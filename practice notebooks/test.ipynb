{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \\                                           question  \\\n",
      "0  0   Give a definition for the term \"artificial ne...   \n",
      "1  1   Give a definition for the term \"artificial ne...   \n",
      "2  2   Give a definition for the term \"artificial ne...   \n",
      "3  3   Give a definition for the term \"artificial ne...   \n",
      "4  4   Give a definition for the term \"artificial ne...   \n",
      "\n",
      "                                      student_answer  grades_round  \\\n",
      "0  An artificial neural network is a massively pa...             2   \n",
      "1  Artificial neural network consists of: . Large...             2   \n",
      "2  An artificial neural network is a massive dist...             1   \n",
      "3  An ANN is a layered graphical model containing...             2   \n",
      "4  Artificial Neural Networks are large parallel ...             2   \n",
      "\n",
      "                                    student_modified  \\\n",
      "0  artificial neural network massively parallel d...   \n",
      "1  artificial neural network consists largely par...   \n",
      "2  artificial neural network massive distributed ...   \n",
      "3  ann layered graphical model containing neuron ...   \n",
      "4  artificial neural network large parallel proce...   \n",
      "\n",
      "                                          ref_answer  \\\n",
      "0  A neural network is a massively parallel distr...   \n",
      "1  A neural network is a massively parallel distr...   \n",
      "2  A neural network is a massively parallel distr...   \n",
      "3  A neural network is a massively parallel distr...   \n",
      "4  A neural network is a massively parallel distr...   \n",
      "\n",
      "                                         qn_modified  \\\n",
      "0  give definition term artificial neural network...   \n",
      "1  give definition term artificial neural network...   \n",
      "2  give definition term artificial neural network...   \n",
      "3  give definition term artificial neural network...   \n",
      "4  give definition term artificial neural network...   \n",
      "\n",
      "                                        ref_modified  \\\n",
      "0  neural network massively parallel distributed ...   \n",
      "1  neural network massively parallel distributed ...   \n",
      "2  neural network massively parallel distributed ...   \n",
      "3  neural network massively parallel distributed ...   \n",
      "4  neural network massively parallel distributed ...   \n",
      "\n",
      "                                     student_demoted  \\\n",
      "0  massively parallel distributed processor simpl...   \n",
      "1  consists largely parallel distributed processo...   \n",
      "2  massive distributed processor consists several...   \n",
      "3  ann layered graphical model containing neuron ...   \n",
      "4  large parallel processing unit natural ability...   \n",
      "\n",
      "                                         ref_demoted  ...  \\\n",
      "0  massively parallel distributed processor made ...  ...   \n",
      "1  massively parallel distributed processor made ...  ...   \n",
      "2  massively parallel distributed processor made ...  ...   \n",
      "3  massively parallel distributed processor made ...  ...   \n",
      "4  massively parallel distributed processor made ...  ...   \n",
      "\n",
      "                                          embed_stud  \\\n",
      "0  [[ 2.2006836   0.86382484  0.27182007  2.55627...   \n",
      "1  [[ 1.33543945  1.09904457  0.52998901  2.03334...   \n",
      "2  [[ 0.41577148 -0.37836266  0.22351074  0.95300...   \n",
      "3  [[ 2.1478271e+00  1.4641495e+00 -3.6404419e-01...   \n",
      "4  [[ 8.80483398e-01  1.30450607e+00 -4.42072144e...   \n",
      "\n",
      "                                   embed_ref_demoted  \\\n",
      "0  [[ 1.6300049e+00  1.5985355e+00 -1.2829590e-01...   \n",
      "1  [[ 1.6300049e+00  1.5985355e+00 -1.2829590e-01...   \n",
      "2  [[ 1.6300049e+00  1.5985355e+00 -1.2829590e-01...   \n",
      "3  [[ 1.6300049e+00  1.5985355e+00 -1.2829590e-01...   \n",
      "4  [[ 1.6300049e+00  1.5985355e+00 -1.2829590e-01...   \n",
      "\n",
      "                                  embed_stud_demoted  \\\n",
      "0  [[ 2.0412598e+00  4.9321938e-01  1.0058594e-01...   \n",
      "1  [[ 1.19566895  0.7539518   0.13561035  1.22295...   \n",
      "2  [[ 3.84277344e-01 -4.89446640e-01  1.72241211e...   \n",
      "3  [[ 1.97546387e+00  1.12967682e+00 -6.56402588e...   \n",
      "4  [[ 0.68797852  0.72406174 -0.85735535  1.14616...   \n",
      "\n",
      "                                             aligned  \\\n",
      "0  [['neural', 'neural'], ['network', 'network'],...   \n",
      "1  [['knowledge', 'knowledge'], ['parallel', 'par...   \n",
      "2  [['knowledge', 'knowledge'], ['neural', 'neura...   \n",
      "3  [['resemble', 'resembling'], ['neural', 'neuro...   \n",
      "4  [['knowledge', 'knowledge'], ['processing', 'p...   \n",
      "\n",
      "                                     aligned_demoted cos_similarity  \\\n",
      "0  [['simple', 'simple'], ['processing', 'process...       0.947867   \n",
      "1  [['knowledge', 'knowledge'], ['knowledge', 'kn...       0.964398   \n",
      "2  [['knowledge', 'knowledge'], ['distributed', '...       0.854767   \n",
      "3  [['environment', 'environment'], ['learning', ...       0.788166   \n",
      "4  [['knowledge', 'knowledge'], ['processing', 'p...       0.894408   \n",
      "\n",
      "  cos_similarity_demo  aligned_score  aligned_score_demo  question_id  \n",
      "0            0.933466       0.969697            0.950888            1  \n",
      "1            0.951182       0.883259            0.818713            1  \n",
      "2            0.775333       0.498039            0.465632            1  \n",
      "3            0.735229       0.322950            0.220386            1  \n",
      "4            0.828665       0.585639            0.482094            1  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"asag_dataset.csv\")  \n",
    "\n",
    "# Display first few rows\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shawn\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer('paraphrase-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to compute SBERT embeddings\n",
    "# def compute_embedding(text):\n",
    "#     return model.encode(text) if isinstance(text, str) else np.zeros(384)  # 384 is SBERT's embedding size\n",
    "\n",
    "# # Apply the function to the relevant columns\n",
    "# df['embed_ref'] = df['ref_answer'].apply(compute_embedding)\n",
    "# df['embed_stud'] = df['student_answer'].apply(compute_embedding)\n",
    "# df['embed_ref_modified'] = df['ref_modified'].apply(compute_embedding)\n",
    "# df['embed_stud_modified'] = df['student_modified'].apply(compute_embedding)\n",
    "# df['embed_ref_demoted'] = df['ref_demoted'].apply(compute_embedding)\n",
    "# df['embed_stud_demoted'] = df['student_demoted'].apply(compute_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['\\', 'question', 'student_answer', 'grades_round', 'student_modified',\n",
      "       'ref_answer', 'qn_modified', 'ref_modified', 'student_demoted',\n",
      "       'ref_demoted', 'length_ratio', 'embed_ref', 'embed_stud',\n",
      "       'embed_ref_demoted', 'embed_stud_demoted', 'aligned', 'aligned_demoted',\n",
      "       'cos_similarity', 'cos_similarity_demo', 'aligned_score',\n",
      "       'aligned_score_demo', 'question_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Columns: ['embed_ref_modified', 'embed_stud_modified', 'cos_similarity_modified']\n"
     ]
    }
   ],
   "source": [
    "expected_columns = [\n",
    "    \"question\", \"student_answer\", \"ref_answer\", \"qn_modified\", \"student_modified\",\n",
    "    \"ref_modified\", \"student_demoted\", \"ref_demoted\", \"length_ratio\",\n",
    "    \"embed_ref\", \"embed_stud\", \"embed_ref_modified\", \"embed_stud_modified\",\n",
    "    \"embed_ref_demoted\", \"embed_stud_demoted\", \"aligned\", \"aligned_demoted\",\n",
    "    \"grades_round\", \"cos_similarity\", \"cos_similarity_modified\", \"cos_similarity_demo\",\n",
    "    \"aligned_score\", \"aligned_score_demo\", \"question_id\"\n",
    "]\n",
    "\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "print(\"Missing Columns:\", missing_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all values are strings and handle NaNs\n",
    "df[\"embed_ref_modified\"] = df[\"ref_modified\"].astype(str).apply(lambda x: sbert_model.encode(x).tolist() if x.strip() else None)\n",
    "df[\"embed_stud_modified\"] = df[\"student_modified\"].astype(str).apply(lambda x: sbert_model.encode(x).tolist() if x.strip() else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings back to NumPy arrays and compute cosine similarity\n",
    "df['cos_similarity_modified'] = df.apply(\n",
    "    lambda row: cosine_similarity([row['embed_ref_modified']], [row['embed_stud_modified']])[0][0],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Columns: []\n"
     ]
    }
   ],
   "source": [
    "# #CHECK AGAIN\n",
    "# expected_columns = [\n",
    "#     \"question\", \"student_answer\", \"ref_answer\", \"qn_modified\", \"student_modified\",\n",
    "#     \"ref_modified\", \"student_demoted\", \"ref_demoted\", \"length_ratio\",\n",
    "#     \"embed_ref\", \"embed_stud\", \"embed_ref_modified\", \"embed_stud_modified\",\n",
    "#     \"embed_ref_demoted\", \"embed_stud_demoted\", \"aligned\", \"aligned_demoted\",\n",
    "#     \"grades_round\", \"cos_similarity\", \"cos_similarity_modified\", \"cos_similarity_demo\",\n",
    "#     \"aligned_score\", \"aligned_score_demo\", \"question_id\"\n",
    "# ]\n",
    "\n",
    "# missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "# print(\"Missing Columns:\", missing_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \\                                           question  \\\n",
      "0  0   Give a definition for the term \"artificial ne...   \n",
      "1  1   Give a definition for the term \"artificial ne...   \n",
      "2  2   Give a definition for the term \"artificial ne...   \n",
      "3  3   Give a definition for the term \"artificial ne...   \n",
      "4  4   Give a definition for the term \"artificial ne...   \n",
      "\n",
      "                                      student_answer  grades_round  \\\n",
      "0  An artificial neural network is a massively pa...             2   \n",
      "1  Artificial neural network consists of: . Large...             2   \n",
      "2  An artificial neural network is a massive dist...             1   \n",
      "3  An ANN is a layered graphical model containing...             2   \n",
      "4  Artificial Neural Networks are large parallel ...             2   \n",
      "\n",
      "                                    student_modified  \\\n",
      "0  artificial neural network massively parallel d...   \n",
      "1  artificial neural network consists largely par...   \n",
      "2  artificial neural network massive distributed ...   \n",
      "3  ann layered graphical model containing neuron ...   \n",
      "4  artificial neural network large parallel proce...   \n",
      "\n",
      "                                          ref_answer  \\\n",
      "0  A neural network is a massively parallel distr...   \n",
      "1  A neural network is a massively parallel distr...   \n",
      "2  A neural network is a massively parallel distr...   \n",
      "3  A neural network is a massively parallel distr...   \n",
      "4  A neural network is a massively parallel distr...   \n",
      "\n",
      "                                         qn_modified  \\\n",
      "0  give definition term artificial neural network...   \n",
      "1  give definition term artificial neural network...   \n",
      "2  give definition term artificial neural network...   \n",
      "3  give definition term artificial neural network...   \n",
      "4  give definition term artificial neural network...   \n",
      "\n",
      "                                        ref_modified  \\\n",
      "0  neural network massively parallel distributed ...   \n",
      "1  neural network massively parallel distributed ...   \n",
      "2  neural network massively parallel distributed ...   \n",
      "3  neural network massively parallel distributed ...   \n",
      "4  neural network massively parallel distributed ...   \n",
      "\n",
      "                                     student_demoted  \\\n",
      "0  massively parallel distributed processor simpl...   \n",
      "1  consists largely parallel distributed processo...   \n",
      "2  massive distributed processor consists several...   \n",
      "3  ann layered graphical model containing neuron ...   \n",
      "4  large parallel processing unit natural ability...   \n",
      "\n",
      "                                         ref_demoted  ...  \\\n",
      "0  massively parallel distributed processor made ...  ...   \n",
      "1  massively parallel distributed processor made ...  ...   \n",
      "2  massively parallel distributed processor made ...  ...   \n",
      "3  massively parallel distributed processor made ...  ...   \n",
      "4  massively parallel distributed processor made ...  ...   \n",
      "\n",
      "                                             aligned  \\\n",
      "0  [['neural', 'neural'], ['network', 'network'],...   \n",
      "1  [['knowledge', 'knowledge'], ['parallel', 'par...   \n",
      "2  [['knowledge', 'knowledge'], ['neural', 'neura...   \n",
      "3  [['resemble', 'resembling'], ['neural', 'neuro...   \n",
      "4  [['knowledge', 'knowledge'], ['processing', 'p...   \n",
      "\n",
      "                                     aligned_demoted cos_similarity  \\\n",
      "0  [['simple', 'simple'], ['processing', 'process...       0.947867   \n",
      "1  [['knowledge', 'knowledge'], ['knowledge', 'kn...       0.964398   \n",
      "2  [['knowledge', 'knowledge'], ['distributed', '...       0.854767   \n",
      "3  [['environment', 'environment'], ['learning', ...       0.788166   \n",
      "4  [['knowledge', 'knowledge'], ['processing', 'p...       0.894408   \n",
      "\n",
      "  cos_similarity_demo aligned_score aligned_score_demo question_id  \\\n",
      "0            0.933466      0.969697           0.950888           1   \n",
      "1            0.951182      0.883259           0.818713           1   \n",
      "2            0.775333      0.498039           0.465632           1   \n",
      "3            0.735229      0.322950           0.220386           1   \n",
      "4            0.828665      0.585639           0.482094           1   \n",
      "\n",
      "                                  embed_ref_modified  \\\n",
      "0  [0.021921474486589432, 0.031473830342292786, 0...   \n",
      "1  [0.021921474486589432, 0.031473830342292786, 0...   \n",
      "2  [0.021921474486589432, 0.031473830342292786, 0...   \n",
      "3  [0.021921474486589432, 0.031473830342292786, 0...   \n",
      "4  [0.021921474486589432, 0.031473830342292786, 0...   \n",
      "\n",
      "                                 embed_stud_modified  cos_similarity_modified  \n",
      "0  [-0.0573633573949337, 0.016812322661280632, -0...                 0.927186  \n",
      "1  [0.039404336363077164, -0.0275457464158535, 0....                 0.893491  \n",
      "2  [-0.044750820845365524, 0.17625218629837036, 0...                 0.761908  \n",
      "3  [0.040430035442113876, -0.2679974436759949, 0....                 0.490689  \n",
      "4  [-0.030009662732481956, -0.2501882016658783, 0...                 0.679526  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Assuming your DataFrame is called 'df'\n",
    "# df.to_csv('new_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_grades(cos_sim):\n",
    "    if cos_sim > 0.69:\n",
    "        return \"Completely Correct\", 2\n",
    "    elif cos_sim > 0.395998348:\n",
    "        return \"Partially Incorrect\", 1\n",
    "    else:\n",
    "        return \"Incorrect\", 0\n",
    "\n",
    "df[[\"grade_text\", \"grades_auto\"]] = df[\"cos_similarity_modified\"].apply(lambda x: pd.Series(assign_grades(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('new_file_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shawn\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Compute BERTScore\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m P, R, F1 \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstudent_modified\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mref_modified\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroberta-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F1\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bert_score\\score.py:123\u001b[0m, in \u001b[0;36mscore\u001b[1;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalculating scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m--> 123\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m \u001b[43mbert_cos_score_idf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43midf_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ref_group_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     max_preds \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bert_score\\utils.py:607\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[1;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(l)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 607\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mdedup_and_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m embs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    609\u001b[0m iter_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size)\n",
      "File \u001b[1;32mc:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bert_score\\utils.py:605\u001b[0m, in \u001b[0;36mbert_cos_score_idf.<locals>.dedup_and_sort\u001b[1;34m(l)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bert_score\\utils.py:605\u001b[0m, in \u001b[0;36mbert_cos_score_idf.<locals>.dedup_and_sort.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(l)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(df[\"student_modified\"].tolist(), df[\"ref_modified\"].tolist(), lang=\"en\", model_type=\"roberta-large\")\n",
    "\n",
    "df[\"bert_score\"] = F1.numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
