7/2/25
Tried various models:

model_para = SentenceTransformer("paraphrase-MiniLM-L6-v2")  # Lightweight version

model_general = SentenceTransformer('all-mpnet-base-v2')

model_sts = SentenceTransformer('stsb-roberta-large')  # Or another stsb-* model

1. Now need to know which model to actually use for my task based on dataset or if I should go to LLMs . OKAY.

2. Do I train a model and keep or Should I run the whole program from the beginning when trying to start the application?. OKAY

3.  Use Next.js and Snowflake

4.  Make sure to use Gemini API.


10/2/25.

Do steps as GPT says,
Check Gemini Model. if manual grading and automated grading is different, then ask Gemini and give correct answer
PROPER PROMPT providing.

API - AIzaSyD7bPd7wMyzknp9fJDWmZVS-qX7jXYXFUQ


11/2/25

Do as GPT
Dont need to make a model
Test on new datasets

Make sure I use snowflake, flask, create 


13/2/25

graded answers test
put bard on edge cases
put error and check,
also remove first column and grades round

question id match from whatever was given.

14/2/25
FINETUNING Combined similarity


okay, i have combined and used this

df["combined_similarity"] = (0.4 * df["cos_similarity_demo"]) + (0.6 * df["cos_similarity_modified"])

def assign_grades(cos_sim):
    if cos_sim > 0.7834:
        return "Completely Correct", 2
    elif cos_sim > 0.3126:
        return "Partially Incorrect", 1
    else:
        return "Incorrect", 0

df[["grade_text", "grades_auto"]] = df["combined_similarity"].apply(lambda x: pd.Series(assign_grades(x)))

Now, I want to test on new question and answers and assign a grade for it 2/1/0. It should check if the question is matched then based on the same question_id it should also check for the ref_answer.
All these required features must be calculated,
Index(['question', 'student_answer', 'student_modified', 'ref_answer',
       'qn_modified', 'ref_modified', 'student_demoted', 'ref_demoted',
       'length_ratio', 'embed_ref', 'embed_stud', 'embed_ref_demoted',
       'embed_stud_demoted', 'aligned', 'aligned_demoted', 'cos_similarity',
       'cos_similarity_demo', 'aligned_score', 'aligned_score_demo',
       'question_id', 'embed_ref_modified', 'embed_stud_modified',
       'cos_similarity_modified', 'combined_similarity', 'grade_text',
       'grades_auto'],
      dtype='object')

- The columns were calculated based on this:
question: the original question of the exam
student_answer : students answer
grades_round: grade for the student's answer
student_modified: student's answer after NLP preprocessing like lowercase conversion and lemmatization
ref_answer: correct answer for the question
qn_modified: question after NLP preprocessing like lowercase conversion and lemmatization
ref_modified: correct answer after NLP preprocessing like lowercase conversion and lemmatization
student_demoted: student's answer after removing words from the corresponding question
ref_demoted: correct answer after removing words from the corresponding question
length_ratio: student answer length divided by the reference answer length
embed_ref: word2vec embedding of reference answer
embed_stud: word2vec embedding of student answer
embed_ref_demoted: word2vec embedding of demoted reference answer
embed_stud_demoted: word2vec embedding of demoted student answer
aligned: word alignment pairs between student answer and reference answer
aligned_demoted: word alignment pairs between the demoted versions of student and reference answer
cos_similarity: cosine similarity between the embeddings of student and reference answer
cos_similarity_demo: cosine similarity between the embeddings of demoted versions of student and reference answer
aligned_score: word alignment score between student answer and reference answer
aligned_score_demo: word alignment score between the demoted versions of student and reference answer
question_id: question number


Also if the combined_similarity is coming in the range
 0.69 - 0.81 and 0.27 - 0.49 , then write a prompt to call Bard LLM and ask it what grade to score, it should answer in format..
" 2 | Completely Correct ...." or " 1 | Partially Correct ...." or " 0 | Incorrect...."  so it is easier to extract the information


Later I want to store all these data in snowflake and any new coming answers should also be stored in snowflake, for that I need to create a frontend, backend and make it pretty using flask and react

15/2/25

Wrote the prompt, now test against various answers

make frontend,backend, database,
KPI's length, keywords, correct answers

17/2/25

CUDA
KPI's


4️⃣ Optimize Speed for Large-Scale Grading
Right now, SBERT + FastText + Gemini can be slow when grading thousands of responses.
Solution:
Use Faiss Indexing to precompute reference embeddings (so you don’t recalculate them every time).
Process multiple answers in parallel (e.g., using multiprocessing or Ray).
➡ Do you need help optimizing speed for bulk grading?

I will need this

Started with UI.
This error:
PS C:\Users\shawn\OneDrive\Desktop\Final Sem Project\frontend> npm list tailwindcss
frontend@0.1.0 C:\Users\shawn\OneDrive\Desktop\Final Sem Project\frontend
├─┬ react-scripts@5.0.1
│ └── tailwindcss@3.4.17
└── tailwindcss@4.0.6


Tailwind not working inside frontend, cant create index.css


18/2/25

worked on UI/UX, using claude, perplexity, gpt
change navbar ui/ux

19/2/25
Home screen change photo(animation)
Edit pdf upload screen.
Add Back key
Where to add python
Add Photo image (if needed) / display Previously Uploaded PDFs based on SQL
maybe faster using GPU?
put code in gpt

20/2/25
Where to add python
Add Photo image (if needed) / display Previously Uploaded PDFs based on SQL
maybe faster using GPU?

Put nn.py, tell that is the logic we are going to use. and if we need to store it in a different py file and call it? then KPIs

Total marks out of number of questions * 2
plotly bar graph and line chart for 
% of questions got correct
% of questions partially correct
% of questions incorrect
Question, Student Answer, Reference Answer, Important keywords ask Gemini to grade each answer and give feedback,
download button as csv

OCR

24/2/25

Extraction of Question properly, Why Question 1 and 3 not being extracted the way it should. Then Use KPIs

28/2/25
Make PPT
Make OCR
Make Bulk Grading
Display PDF? Change Table to Scrollable
Line chart edit
Download Button for PDF


1/3/25

AI detection


--Testing--


Correct matching of questions even if wrong order.

missing questions and answers.

mock functionality for the grading page table if needed.

About Us Page. (MAKE)


